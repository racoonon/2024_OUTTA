{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# %cd /content/drive/MyDrive/2024_OUTTA_bootcamp/deeplearning_basic/P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <A549E5FA-1487-3474-A747-4913D621982E> /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <A84DFEFF-287E-3B94-A7DB-731FA5F9CBBC> /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 임포트\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models import VGG16_Weights\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "args = {\n",
    "    \"train_path\" : \"dataset/train.csv\",\n",
    "    \"test_path\" : \"dataset/test.csv\",\n",
    "    \"submit_path\" : \"dataset/sample_submission.csv\",\n",
    "    \"batch_size\" : 64,\n",
    "    \"epochs\" : 1,\n",
    "    \"lr\" : 2e-5,\n",
    "    \"seed_val\" : 42     # 절대 수정하지 마세요.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤시드 고정하기\n",
    "seed = args[\"seed_val\"]\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available() : \n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# 디바이스 선택\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.5.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchsummary ## model을 요약하기 위해 torchsummary 설치\n",
    "from torchsummary import summary as summary_## 모델 정보를 확인하기 위해 torchsummary 함수 import\n",
    "\n",
    "## 모델의 형태를 출력하기 위한 함수 \n",
    "def summary_model(model, input_shape=(3,32,32)):\n",
    "    model = model.to(device)\n",
    "    summary_(model, input_shape) ## (model, (input shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. 데이터셋**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(1) 데이터 불러오기 및 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 테스트 데이터 로드\n",
    "train = pd.read_csv(args['train_path'])\n",
    "test = pd.read_csv(args['test_path'])\n",
    "\n",
    "# (1-1) 훈련 데이터에서 특징(x_train)과 라벨(y_train) 분리\n",
    "x_train = train.iloc[:, 1:]  # 첫 번째 열을 제외한 모든 열 (특징)\n",
    "y_train = train['label']     # 'label' 열 (라벨)\n",
    "x_test = test                # 테스트 데이터\n",
    "\n",
    "# (1-2) numpy 배열로 변환 후, torch Tensor로 변환\n",
    "x_train = torch.Tensor(np.array(x_train))\n",
    "y_train = torch.LongTensor(np.array(y_train))\n",
    "x_test = torch.Tensor(np.array(x_test))\n",
    "\n",
    "# (1-3) 데이터를 (N, H, W, C) 형태로 재구성\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# (1-4) 데이터를 (N, C, H, W) 형태로 변환\n",
    "x_train = x_train.permute(0, 3, 1, 2)\n",
    "x_test = x_test.permute(0, 3, 1, 2)\n",
    "\n",
    "# (1-5) 데이터를 (N, 3, 28, 28) 형태로 변환 (채널을 3으로 복제)\n",
    "x_train = x_train.expand(-1, 3, -1, -1)\n",
    "x_test = x_test.expand(-1, 3, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27455, 3, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7172, 3, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(2) 데이터셋과 데이터로더**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 테스트 데이터셋 로드\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test)\n",
    "\n",
    "# 데이터로더 정의\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args[\"batch_size\"], shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. 모델**\n",
    "사전학습된 VGG16을 사용하여 프로젝트를 수행하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(1) VGG16 특징 추출기를 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "              ReLU-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,928\n",
      "              ReLU-4           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-5           [-1, 64, 16, 16]               0\n",
      "            Conv2d-6          [-1, 128, 16, 16]          73,856\n",
      "              ReLU-7          [-1, 128, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]         147,584\n",
      "              ReLU-9          [-1, 128, 16, 16]               0\n",
      "        MaxPool2d-10            [-1, 128, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]         295,168\n",
      "             ReLU-12            [-1, 256, 8, 8]               0\n",
      "           Conv2d-13            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-14            [-1, 256, 8, 8]               0\n",
      "           Conv2d-15            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-16            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-17            [-1, 256, 4, 4]               0\n",
      "           Conv2d-18            [-1, 512, 4, 4]       1,180,160\n",
      "             ReLU-19            [-1, 512, 4, 4]               0\n",
      "           Conv2d-20            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-21            [-1, 512, 4, 4]               0\n",
      "           Conv2d-22            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-23            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-24            [-1, 512, 2, 2]               0\n",
      "           Conv2d-25            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-26            [-1, 512, 2, 2]               0\n",
      "           Conv2d-27            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-28            [-1, 512, 2, 2]               0\n",
      "           Conv2d-29            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-30            [-1, 512, 2, 2]               0\n",
      "        MaxPool2d-31            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.46\n",
      "Params size (MB): 56.13\n",
      "Estimated Total Size (MB): 60.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# (3-1) VGG16 모델 로드 및 특징 추출 부분 사용\n",
    "# 가중치는 'VGG16_Weights.IMAGENET1K_V1' 사용하세요.\n",
    "model = torchvision.models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features\n",
    "\n",
    "# (3-2) 모델에 글로벌 평균 풀링 계층 추가\n",
    "model.global_avg_pool2d = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "# 모델 요약 정보 출력\n",
    "summary_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "              ReLU-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,928\n",
      "              ReLU-4           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-5           [-1, 64, 16, 16]               0\n",
      "            Conv2d-6          [-1, 128, 16, 16]          73,856\n",
      "              ReLU-7          [-1, 128, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]         147,584\n",
      "              ReLU-9          [-1, 128, 16, 16]               0\n",
      "        MaxPool2d-10            [-1, 128, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]         295,168\n",
      "             ReLU-12            [-1, 256, 8, 8]               0\n",
      "           Conv2d-13            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-14            [-1, 256, 8, 8]               0\n",
      "           Conv2d-15            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-16            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-17            [-1, 256, 4, 4]               0\n",
      "           Conv2d-18            [-1, 512, 4, 4]       1,180,160\n",
      "             ReLU-19            [-1, 512, 4, 4]               0\n",
      "           Conv2d-20            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-21            [-1, 512, 4, 4]               0\n",
      "           Conv2d-22            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-23            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-24            [-1, 512, 2, 2]               0\n",
      "           Conv2d-25            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-26            [-1, 512, 2, 2]               0\n",
      "           Conv2d-27            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-28            [-1, 512, 2, 2]               0\n",
      "           Conv2d-29            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-30            [-1, 512, 2, 2]               0\n",
      "        MaxPool2d-31            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.46\n",
      "Params size (MB): 56.13\n",
      "Estimated Total Size (MB): 60.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary_(model, (3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(2) VGG16 특징 추출기를 동결시키기**\n",
    "1. 계산 비용 절감:\n",
    "    - 모델의 일부 파라미터를 동결하면 그 부분은 학습되지 않기 때문에 계산량이 줄어듭니다. \n",
    "\n",
    "    - 이는 학습 속도를 빠르게 하고, GPU 메모리 사용량을 줄이는 데 도움이 됩니다.\n",
    "\n",
    "2. 오버피팅 방지:\n",
    "    - 작은 데이터셋을 사용할 때 모델이 쉽게 오버피팅될 수 있습니다. \n",
    "    \n",
    "    - 이미 학습된 파라미터를 동결하면 모델이 새로운 데이터에 맞춰 과도하게 학습되는 것을 방지할 수 있습니다.\n",
    "\n",
    "\n",
    "3. 전이 학습(Transfer Learning):\n",
    "    - 사전 학습된 모델(예: ImageNet 데이터셋으로 학습된 모델)의 초반 레이어는 일반적인 특징(예: 에지, 텍스처 등)을 잘 잡아냅니다. \n",
    "\n",
    "    - 이러한 특징을 이용하면 새로운 데이터셋에서도 좋은 성능을 낼 수 있습니다. \n",
    "\n",
    "    - 모델의 후반 레이어만 재학습하여 새로운 데이터셋에 맞출 수 있습니다.\n",
    "\n",
    "4. 더 빠른 수렴:\n",
    "    - 동결된 파라미터는 변화하지 않으므로 모델이 더 빠르게 수렴할 수 있습니다. \n",
    "    \n",
    "    - 이는 전체 학습 시간을 단축시키는 데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "              ReLU-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,928\n",
      "              ReLU-4           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-5           [-1, 64, 16, 16]               0\n",
      "            Conv2d-6          [-1, 128, 16, 16]          73,856\n",
      "              ReLU-7          [-1, 128, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]         147,584\n",
      "              ReLU-9          [-1, 128, 16, 16]               0\n",
      "        MaxPool2d-10            [-1, 128, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]         295,168\n",
      "             ReLU-12            [-1, 256, 8, 8]               0\n",
      "           Conv2d-13            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-14            [-1, 256, 8, 8]               0\n",
      "           Conv2d-15            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-16            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-17            [-1, 256, 4, 4]               0\n",
      "           Conv2d-18            [-1, 512, 4, 4]       1,180,160\n",
      "             ReLU-19            [-1, 512, 4, 4]               0\n",
      "           Conv2d-20            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-21            [-1, 512, 4, 4]               0\n",
      "           Conv2d-22            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-23            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-24            [-1, 512, 2, 2]               0\n",
      "           Conv2d-25            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-26            [-1, 512, 2, 2]               0\n",
      "           Conv2d-27            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-28            [-1, 512, 2, 2]               0\n",
      "           Conv2d-29            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-30            [-1, 512, 2, 2]               0\n",
      "        MaxPool2d-31            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 7,079,424\n",
      "Non-trainable params: 7,635,264\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.46\n",
      "Params size (MB): 56.13\n",
      "Estimated Total Size (MB): 60.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 모델의 앞부분 파라미터 고정 (동결)\n",
    "for para in model[:-8].parameters(): \n",
    "    para.requires_grad = False\n",
    "\n",
    "# 파라미터 동결 후 모델 요약 정보 출력\n",
    "summary_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(3) VGG16 분류기를 만들기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "              ReLU-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,928\n",
      "              ReLU-4           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-5           [-1, 64, 16, 16]               0\n",
      "            Conv2d-6          [-1, 128, 16, 16]          73,856\n",
      "              ReLU-7          [-1, 128, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]         147,584\n",
      "              ReLU-9          [-1, 128, 16, 16]               0\n",
      "        MaxPool2d-10            [-1, 128, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]         295,168\n",
      "             ReLU-12            [-1, 256, 8, 8]               0\n",
      "           Conv2d-13            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-14            [-1, 256, 8, 8]               0\n",
      "           Conv2d-15            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-16            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-17            [-1, 256, 4, 4]               0\n",
      "           Conv2d-18            [-1, 512, 4, 4]       1,180,160\n",
      "             ReLU-19            [-1, 512, 4, 4]               0\n",
      "           Conv2d-20            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-21            [-1, 512, 4, 4]               0\n",
      "           Conv2d-22            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-23            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-24            [-1, 512, 2, 2]               0\n",
      "           Conv2d-25            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-26            [-1, 512, 2, 2]               0\n",
      "           Conv2d-27            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-28            [-1, 512, 2, 2]               0\n",
      "           Conv2d-29            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-30            [-1, 512, 2, 2]               0\n",
      "        MaxPool2d-31            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 1, 1]               0\n",
      "          Flatten-33                  [-1, 512]               0\n",
      "           Linear-34                   [-1, 25]          12,825\n",
      "================================================================\n",
      "Total params: 14,727,513\n",
      "Trainable params: 7,092,249\n",
      "Non-trainable params: 7,635,264\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.47\n",
      "Params size (MB): 56.18\n",
      "Estimated Total Size (MB): 60.66\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# (4-1) 새로운 분류기 정의\n",
    "classifier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512, 25),\n",
    ")\n",
    "\n",
    "# (4-2) 모델에 새로운 분류기 추가\n",
    "model.classifier = classifier\n",
    "\n",
    "# 모델을 장치로 이동\n",
    "model = model.to(device)\n",
    "\n",
    "# 최종 모델 요약 정보 출력\n",
    "summary_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[32]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# VGG16에서 사용한 transform을 이용\n",
    "transform = VGG16_Weights.DEFAULT.transforms(antialias=False)\n",
    "\n",
    "# resize 크기는 32가 되도록 설정\n",
    "transform.resize_size=[32]\n",
    "\n",
    "# transform 확인하기\n",
    "print(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, model, device, args):\n",
    "    \"\"\"\n",
    "    주어진 데이터로 모델을 학습시키는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        train_dataloader (DataLoader): 학습 데이터를 제공하는 DataLoader 객체\n",
    "        valid_dataloader (DataLoader): 검증 데이터를 제공하는 DataLoader 객체\n",
    "        model (torch.nn.Module): 학습할 모델\n",
    "        device (torch.device): 사용할 디바이스 (CPU 또는 GPU)\n",
    "        args (dict): 학습 관련 인자들을 포함한 딕셔너리\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # (5-1) Adam 옵티마이저와 교차 엔트로피 손실 함수 정의\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    model.zero_grad()  # 모델의 그래디언트 초기화\n",
    "    \n",
    "    for epoch in range(args[\"epochs\"]):\n",
    "        model.train()       # 모델을 훈련 모드로 설정\n",
    "        \n",
    "        total_loss = 0      # 전체 손실 초기화\n",
    "        total_accuracy = 0  # 전체 정확도 초기화\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{args[\"epochs\"]}')\n",
    "        \n",
    "        for image, label in tqdm(train_dataloader):\n",
    "            image = transform(image).to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # (5-2) 모델을 사용하여 예측 수행\n",
    "            pred = model(image)\n",
    "            \n",
    "            # (5-3) 손실 계산 및 누적\n",
    "            loss = loss_fn(pred, label)\n",
    "            \n",
    "            # (5-4) 역전파를 통해 기울기 계산\n",
    "            loss.backward()\n",
    "            \n",
    "            # (5-5) 파라미터 업데이트\n",
    "            optimizer.step()\n",
    "            \n",
    "            # (5-6) 모델의 그래디언트 초기화\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # loss 계산\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # accaracy 계산\n",
    "            label = label.cpu()\n",
    "            pred = pred.argmax(dim = 1).cpu()\n",
    "            accuracy = accuracy_score(label, pred)\n",
    "            total_accuracy += accuracy\n",
    "        \n",
    "        # 평균 손실과 정확도 계산\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        avg_accuracy = total_accuracy / len(train_dataloader)\n",
    "        \n",
    "        # 모델 체크포인트 저장\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model': model,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item,\n",
    "            }, f'./results/model_state_dict_epoch_{epoch+1}.pth')\n",
    "        \n",
    "        # 현재 에포크의 손실과 정확도 출력\n",
    "        print(f'CheckPoint : model_state_dict_epoch_{epoch+1}.pth')\n",
    "        print(f'train_loss : {avg_loss}, train_acc : {avg_accuracy}\\n')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(train_dataloader, model, device, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. 평가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataloader, model, device):\n",
    "    \"\"\"\n",
    "    모델의 테스트를 수행하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        test_dataloader (DataLoader): 테스트 데이터를 제공하는 DataLoader 객체\n",
    "        model (torch.nn.Module): 평가할 모델\n",
    "        device (torch.device): 사용할 디바이스 (CPU 또는 GPU)\n",
    "\n",
    "    Returns:\n",
    "        preds (list): 각 입력 예시에 대한 모델의 예측 결과 리스트\n",
    "    \"\"\"\n",
    "    model.eval()    # 모델을 평가 모드로 설정\n",
    "    preds = []      # 예측 결과를 저장할 리스트\n",
    "    \n",
    "    # 각 배치에 대해 예측 수행\n",
    "    for image in tqdm(test_dataloader):\n",
    "        image = transform(image[0]).to(device)\n",
    "        \n",
    "        # 기울기 계산을 비활성화하여 예측 수행\n",
    "        with torch.no_grad():\n",
    "            # (6-1) 모델에 입력을 전달하여 예측 수행\n",
    "            output = model(image)\n",
    "            \n",
    "            # (6-2) argmax를 이용하여 예측 결과에서 가장 높은 값의 인덱스를 선택\n",
    "            pred = output.argmax(dim=1)\n",
    "\n",
    "            # (6-3) 예측 결과를 CPU로 이동\n",
    "            pred = pred.cpu()\n",
    "\n",
    "            # (6-4) 예측을 numpy 배열로 변환\n",
    "            pred = pred.numpy()\n",
    "\n",
    "            # (6-5) 예측 결과를 리스트에 추가\n",
    "            preds.append(pred[0])\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 예측값을 얻기 위해 test 함수 호출\n",
    "    preds = test(test_dataloader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(args[\"submit_path\"])\n",
    "submit[\"label\"] = preds\n",
    "submit.to_csv(\"submission_p1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
